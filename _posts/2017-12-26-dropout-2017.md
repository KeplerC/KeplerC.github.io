---
layout: post
title: Automatic Text Scoring By Neural Network
date: 2017-12-25
author: Kaiyuan Chen
catalog: true
tags:
    - Machine Learning
    - Paper Review
    - NLP
---

# Dropout A Simple Way to Prevent Neural Networks from Overfitting
by Srivastava et al.

The key idea of dropout is to randomly drop units (along with their connections) from the neural network during training. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weight.

For larger neural network, we cannot have separately trained net, so we have individual models different from each other and make neural network model different. 

#### Motivation: natural selection & role of sex in evolution 
It can be seen as a **stochastic** regularization techinque, with removing certain nodes at random. 

