<html lang="en" class="liqing-ustc.github.io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	
    <meta name="description" content="Qing Li's Homepage">
    <meta name="author" content="Qing Li">

    <title>Qing Li</title>
    <link rel="icon" href="./assets/QingLi.jpg">
    <!-- Bootstrap Core CSS -->
    <link href="./assets/bootstrap.min.css" rel="stylesheet" type="text/css">

	<!-- Font Awesome -->
	<link href="./assets/font-awesome.min.css" rel="stylesheet" type="text/css">
	
    <!-- Animation -->
	<link href="./assets/animate.css" rel="stylesheet">
	
    <!-- MyTemplate CSS -->
    <link href="./assets/style.css" rel="stylesheet">
	
</head>

<body data-gr-c-s-loaded="true" style="">
	<header id="header-banner">
		<nav class="navbar navbar-default navbar-fixed-top fadeIn top-nav-collapse" role="navigation">
			<div class="container">
				<!-- Brand and toggle get grouped for better mobile display -->
				<div class="navbar-header">
					<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#dropdown-box-1">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					<div class="navbar-brand">
						<a href="http://liqing-ustc.github.io#">Homepage</a>
					</div>
				</div>
				
				<!-- Collect the nav links and other content for toggling -->
				<div class="collapse navbar-collapse navbar-right" id="dropdown-box-1">
					
					<ul class="nav navbar-nav">
						<li class="active"><a href="http://liqing-ustc.github.io#home">HOME</a></li>
						<li><a href="http://liqing-ustc.github.io#aboutus">ABOUT</a></li>
						<li><a href="http://liqing-ustc.github.io#news">NEWS</a></li>
                        <li><a href="http://liqing-ustc.github.io#education">EDUCATION</a></li>
						<li><a href="http://liqing-ustc.github.io#publications">PUBLICATIONS</a></li>
						<li><a href="http://liqing-ustc.github.io#workshops">WORKSHOPS</a></li>
						<li><a href="http://liqing-ustc.github.io#awards">AWARDS</a></li>
                        <li><a href="http://liqing-ustc.github.io#research_experiences">RESEARCH EXPERIENCES</a></li>
					</ul>
					
				</div>
				
			</div> <!-- /.container -->
		</nav> <!-- /.nav -->
	</header>
		
	<!-- banner -->
    <section class="banner" id="home">
		<div class="container" style="margin-top:20px">
			<div class="slogan">
				<h2>QING LI</h2>
				<h4>KEEP CALM AND CARRY ON.</h4>
			</div>
			
			<div class="btn-circle-scroll fadeIn">
				<a href="http://liqing-ustc.github.io#section-footer" class="btn-circle">
					<i class="fa fa-angle-double-down animated"></i>
				</a>
			</div>
			
		</div>
    </section>
	<!-- /.banner -->

	<!-- aboutus -->
    <section class="aboutus" id="aboutus">
		<div class="container">
			<div class="row">
				<div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
					<div class="wow bounceInLeft animated animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
						<h2>About ME</h2> 
					</div>
					<div class="col-sm-3 col-md-3 col-lg-2" style="margin-left:10px">
						<div class="img-aboutus">
							<div class="wow fadeIn animated animated" data-wow-delay="0.2s" style="visibility: visible;-webkit-animation-delay: 0.2s; -moz-animation-delay: 0.2s; animation-delay: 0.2s;">
								<img src="./assets/QingLi.jpg" width="300px" alt="" class="img-responsive img-rounded">
							</div>
						</div>
					</div>							
					<div class="col-sm-9 col-md-9 col-lg-8">
						<h3>
							Qing Li (李庆)
						</h3>
						<p></p>
							<img src="./assets/google_scholar.png" width="15" height="15" alt=""><a href="https://scholar.google.com.sg/citations?user=iwdFZBEAAAAJ&hl=en"> Google Scholar</a> <br>
							<img src="./assets/email.jpg" width="15" height="15" alt=""><a href="mailto:dylan.liqing@gmail.com"> dylan.liqing@gmail.com</a> <br> 
                            <img src="./assets/cv.jpg" width="15" height="15" alt=""><a href="publications/CV.pdf"> Curriculum Vitae</a><br>
                            <img src="./assets/github.svg" width="15" height="15" alt=""><a href="https://github.com/liqing-ustc"> GitHub (liqing-ustc)</a><br>
						<p></p>
						<p>
							I am currently a first-year Ph.D. student in <a href="http://statistics.ucla.edu/">Department of Statistics, University of California, Los Angeles (UCLA)</a>, advised by <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>. My research interests include but are not limited to: vision-and-language (image/video captioning, visual question answering), activity and action recognition in video, deep learning.
						</p>
						
					</div>
				</div>				
			</div>
		</div>
	</section>
	<!-- /.aboutus -->
	<!-- news -->
    <section class="testimonials" id="news">
		<div class="container">
			<div class="wow bounceInLeft animated animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>News</h2> 
			</div>
			<!-- row -->
			<div class="row">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-12">
						<ul>	
							<li><p><strong>2019.02.</strong> One paper is accepted by CVPR 2019: <i>"Detecting Private Information and Its Purpose in Pictures Taken by Blind People."</i> Joint work with <a href="https://www.ischool.utexas.edu/~dannag/AboutMe.html">Prof. Danna Gurari</a> in UT Austin.</p></li>
							<li><p><strong>2019.02.</strong> I am invited to be a reviewer for CVPR 2019, ICCV 2019 and ACL 2019. Be an insightful reviewer!</p></li>
							<li><p><strong>2018.10.</strong> I will attend EMNLP 2018 from Oct. 31 to Nov. 5 in Brussel, Belgium and welcome to approach me and chat!</p></li>
							<li><p><strong>2018.09.</strong> I am invited to be a reviewer for CVPR 2019.</p></li>
							<li><p><strong>2018.09.</strong> I start my PhD at <a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>, University of California, Los Angeles (UCLA)!</p></li>
							<li><p><strong>2018.08.</strong> Our paper <i>"Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions."</i> is accepted by <strong>EMNLP 2018 (Oral)</strong>.</p></li>
							<li><p><strong>2018.07.</strong> Our paper <i>"VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions."</i> is accepted by <strong>ECCV 2018</strong>.</p></li>
							<li><p><strong>2018.03.</strong> I am working as a Research Associate in the University of Texas at Austin, under the supervision of <a href="https://www.ischool.utexas.edu/~dannag/AboutMe.html">Prof. Danna Gurari</a>, from June 25 to August 25, 2018. </p></li>
							<li><p><strong>2018.04.</strong> I will serve as a student volunteer for <a href='http://cvpr2018.thecvf.com/'>CVPR 2018</a> at Salt Lake City, June 18-22.</p></li>
							<li><p><strong>2018.03.</strong> The code for <i>"VizWiz Grand Challenge: Answering Visual Questions from Blind People."</i> is released! Please check below.</p></li>							
							<li><p><strong>2018.03.</strong> Our paper <i>"VizWiz Grand Challenge: Answering Visual Questions from Blind People."</i> is accepted by <strong>CVPR 2018</strong>.</p></li>
                            				<li><p><strong>2018.02.</strong> I will pursue my PhD in <a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>, University of California, Los Angeles (UCLA), from Fall 2018!</p></li>
                            				<li><p><strong>2018.01.</strong> I am visiting the Multimedia Lab in Nanyang Technological University (NTU), Singapore, supervised by <a href="http://www.ntu.edu.sg/home/asjfcai/">Prof. Jianfei Cai</a> and <a href="https://raihanjoty.github.io/">Prof. Shafiq Joty</a>.</p></li>
                        			</ul>
					</div>
				</div>
			</div>
			<!-- ./row -->		
		</div>
	</section>
	<!-- /.news -->
    
    <!-- education -->
    <section class="testimonials" id="education">
		<div class="container">
			<div class="wow bounceInLeft animated animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>Education</h2> 
			</div>
	<!-- row -->
            <div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<!--div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="">
							<img src="./assets/USTC_logo.svg" class="img-responsive img-thumbnail">
						</div>
					</div-->
					<div class="col-sm-9 col-md-9 col-lg-9" style="margin-left:0px">
						<blockquote style="margin-bottom:5px">
							<h4>
								Ph.D. in University of California, Los Angeles (UCLA), 2018.09 - Now
							</h4>
							<p>
								Major: Statistics, Advisor: <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a><br>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
			<!-- row -->
            <div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<!--div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="">
							<img src="./assets/USTC_logo.svg" class="img-responsive img-thumbnail">
						</div>
					</div-->
					<div class="col-sm-9 col-md-9 col-lg-9" style="margin-left:0px">
						<blockquote style="margin-bottom:5px">
							<h4>
								Master in University of Science and Technology of China (USTC), 2015.08 - 2018.07
							</h4>
							<p>
								Major: Information and Communication Engineering, Advisor: <a href="http://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a><br>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
            <div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<!--div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="">
							<img src="./assets/USTC_logo.svg" class="img-responsive img-thumbnail">
						</div>
					</div-->
					<div class="col-sm-9 col-md-9 col-lg-9" style="margin-left:0px">
						<blockquote style="margin-bottom:5px">
							<h4>
								Bachelor in University of Science and Technology of China (USTC), 2011.09 - 2015.07
							</h4>
							<p>
								<!-- Major: Automation, GPA: 3.93/4.3 (92/100), Rank: 1/82 <br> -->
                                Awarded the <i>Guo Moruo Scholarship (郭沫若奖学金)</i>, for the best graduate in Department of Automation.
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
		</div>
	</section>
	<!-- /.education -->
    
    
    
	<!-- publications -->
    <section class="testimonials" id="publications">
		<div class="container">
			<div class="wow bounceInLeft animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>Publications</h2> 
			</div>
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/CVPR19VIZWIZ-Priv.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People.
							</h4>
							<p>
								Danna Gurari, <i><strong>Qing Li</strong></i>, Chi Lin, Yinan Zhao, Anhong Guo, Abigale J. Stangl, Jeffrey P. Bigham. <br>
								IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019<br>
								<a href="" class="label label-success">PDF</a>
								<a href="" class="label label-primary">BibTex</a>
								<a href="https://ivc.ischool.utexas.edu/VizWiz/" class="label label-info">Project</a>
								<a href="" class="label label-info">Code</a>
								
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/ECCV18VQA-E.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions
							</h4>
							<p>
								<i><strong>Qing Li</strong></i>, Qingyi Tao, Shafiq Joty, Jianfei Cai, Jiebo Luo <br>
								IEEE European Conference on Computer Vision (<strong>ECCV</strong>), 2018 <br>
								<a href="https://arxiv.org/abs/1803.07464" class="label label-success">PDF</a>
								<a href="./publications/ECCV18VQA-E.txt" class="label label-primary">BibTex</a>
								<a href="https://github.com/liqing-ustc/VQA-E" class="label label-info">Dataset</a>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/CVPR18VIZWIZ.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								VizWiz Grand Challenge: Answering Visual Questions from Blind People
							</h4>
							<p>
								Danna Gurari, <i><strong>Qing Li</strong></i>, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey Bigham <br>
								IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018 (<strong>Spotlight</strong>) <br>
								<a href="https://arxiv.org/abs/1802.08218" class="label label-success">PDF</a>
								<a href="./publications/CVPR18VIZWIZ.txt" class="label label-primary">BibTex</a>
								<a href="http://vizwiz.org/data/" class="label label-info">Project</a>
								<a href="https://github.com/liqing-ustc/VizWiz_LSTM_CNN_Attention" class="label label-info">Code</a>
								
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/ARXIV18TELL.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions
							</h4>
							<p>
								<i><strong>Qing Li</strong></i>, Jianlong Fu, Dongfei Yu, Tao Mei, Jiebo Luo <br>
								Conference on Empirical Methods in Natural Language Processing (<strong>EMNLP</strong>), 2018 (<strong>Oral</strong>) <br>
								<a href="https://arxiv.org/abs/1801.09041" class="label label-success">PDF</a>
								<a href="./publications/ARXIV18TELL.txt" class="label label-primary">BibTex</a>
								<!-- <a href="https://github.com/liqing-ustc" class="label label-info">Code</a> -->
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->
            <br>
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/IJMIR17ACTION.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Learning Hierarchical Video Representation for Action Recognition
							</h4>
							<p>
								<i><strong>Qing Li</strong></i>, Zhaofan Qiu, Ting Yao, Tao Mei, Yong Rui, Jiebo Luo<br>
								International Journal of Multimedia Information Retrieval (<strong>IJMIR</strong>), February 2017 <br>
								<a href="./publications/IJMIR17ACTION.pdf" class="label label-success">PDF</a>
								<a href="./publications/IJMIR17ACTION.txt" class="label label-primary">BibTex</a>
								<!-- <a href="https://github.com/liqing-ustc" class="label label-info">Code</a> -->
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>            
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/ICMR16ACTION.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation
							</h4>
							<p>
								<i><strong>Qing Li*</strong></i>, Zhaofan Qiu*, Ting Yao, Tao Mei, Yong Rui, Jiebo Luo (*equal contribution) <br>
								ACM International Conference on Multimedia Retrieval (<strong>ICMR</strong>), New YorK, USA, July 2016 (<strong>best paper candidate</strong>) <br>
								<a href="./publications/ICMR16ACTION.pdf" class="label label-success">PDF</a>
								<a href="./publications/ICMR16ACTION.txt" class="label label-primary">BibTex</a>
								<a href="https://people.eecs.berkeley.edu/~lisa_anne/LRCN_video" class="label label-info">Code</a>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->		
		</div>
	</section>
	<!-- /.publications -->
	
	<!-- workshops -->
    <section class="testimonials" id="workshops">
		<div class="container">
			<div class="wow bounceInLeft animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>Workshops</h2> 
			</div>		
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/TRECVID17.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								VIREO @ TRECVID 2017: Video-to-Text, Ad-hoc Video Search and Video Hyperlinking
							</h4>
							<p>
								Phuong Anh Nguyen, <i><strong>Qing Li</strong></i>, Zhi-Qi Cheng, Yi-Jie Lu, Hao Zhang, Xiao Wu, Chong-Wah Ngo. <br>
								NIST TRECVID Workshop (TRECVID'17), Gaithersburg, USA, Nov 2017 <br>
								<a href="./publications/TRECVID17.pdf" class="label label-success">PDF</a>
								<a href="./publications/TRECVID17.txt" class="label label-primary">BibTex</a>
								<a href="http://www-nlpir.nist.gov/projects/trecvid/" class="label label-default">Challenge Homepage</a>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-3 col-md-3 col-lg-2" style="padding-left:0px;padding-right:0px">
						<div class="avatar">
							<img src="./publications/THUMOS15.jpg" class="img-responsive img-thumbnail">
						</div>
					</div>				
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								MSR Asia MSM at THUMOS Challenge 2015
							</h4>
							<p>
								Zhaofan Qiu, <i><strong>Qing Li</strong></i>, Ting Yao, Tao Mei, Yong Rui <br>
								In CVPR THUMOS Challenge Workshop, 2015 (<strong>2nd place in Action Classification task</strong>) <br>
								<a href="./publications/THUMOS15.pdf" class="label label-success">PDF</a>
								<a href="./publications/THUMOS15.txt" class="label label-primary">BibTex</a>
								<a href="http://www.thumos.info/" class="label label-default">Challenge Homepage</a>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->
						
		</div>
	</section>
	<!-- /.workshops -->
	
	<!-- awards -->
    <section class="testimonials" id="awards">
		<div class="container">
			<div class="wow bounceInLeft animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>Awards</h2> 
			</div>
			<!-- row -->
			<div class="row">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-12">
						<ul>
							<li><p><strong>National Scholarship, 2017</strong></p></li>
							<li><p><strong>ICMR’16 Student Travel Grants, 2016</strong></p></li>
						    <li><p><strong>Best Paper Finalist in ICMR'16, 2016</strong></p></li>
							<li><p><strong>Outstanding Graduate in Anhui Province, China, 2015</strong></p></li>
							<li><p><strong><strong><i>Guo Moruo</i></strong> Scholarship (<strong>郭沫若奖学金</strong>), 2014</strong></p></li>
							<li><p><strong>National Scholarship, 2013</strong></p></li>
							<li><p><strong>Outstanding Student Scholarship (Gold Award), 2012</strong></p></li>
						</ul>
					</div>
				</div>
			</div>
			<!-- ./row -->		
		</div>
	</section>
	<!-- /.awards -->
	
    <!-- research experiences -->
    <section class="testimonials" id="research_experiences">
		<div class="container">
			<div class="wow bounceInLeft animated" data-wow-delay="0.1s" style="visibility: visible;-webkit-animation-delay: 0.1s; -moz-animation-delay: 0.1s; animation-delay: 0.1s;"> 
				<h2>Research Experiences</h2> 
			</div>	
			<!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Visual Privacy Recognition (VizWiz-Privacy), 2018.07 - 2018.09
							</h4>
							<p>
								Supervised by Prof. Danna Gurari in University of Texas at Austin
								<ul>
								    <li>Introducing the first visual privacy dataset originating from blind people. For each image, we manually annotate private regions according to a taxonomy that represents privacy concerns relevant to their images. We also annotate whether the private visual information is needed to answer questions asked about the private images.</li>
								    <li>Proposing two tasks to identify (1) whether private information is in an image and (2) whether a question about an image asks about the private content in the image.</li>
                                				</ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Visual Question Answering with Explanation, 2018.01 - 2018.06
							</h4>
							<p>
								Supervised by Prof. Jianfei Cai in NTU, Singapore, and Prof. Jiebo Luo
                                <ul>
                                    <li>Constructed a new dataset of VQA with Explanation (VQA-E), which consists of 181,298 visual questions, answers, and explanations.</li>
                                    <li>Proposed a novel multi-task learning architecture to jointly predict an answer and generate an explanation for the answer. </li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Visual Question Answering for Blind People 2017.10 - 2018.01
							</h4>
							<p>
								Supervised by Prof. Danna Gurari in UT Austin and Prof. Jiebo Luo
                                <ul>
                                    <li>Proposed VizWiz, the first goal-oriented VQA dataset arising from a natural setting. VizWiz consists of 31,000 visual questions originating from blind people.</li>
                                    <li>Analyzed the image-question relevance of VizWiz and benchmarked state-of-the-art VQA algorithms and revealed that VizWiz is a challenging dataset to spur the research on assistive technologies that eliminate accessibility barriers for blind people.  </li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Video Captioning and Ad-hoc Video Search, 2017.02 - 2017.10
							</h4>
							<p>
								Supervised by Prof. Chong-Wah Ngo in City University of Hong Kong
                                <ul>
                                    <li>Proposed a novel framework that can match video and text and generate descriptions for videos by utilizing spatio-temporal attention and applied the proposed framework to the Video to Text task of TRECVID 2017 Competitions.</li>
                                    <li>Revised the framework to search relevant videos given a text query and won 3rd place in the Ad-hoc Video Search task. Our notebook paper is accepted by NIST TRECVID Workshop 2017.</li>
                                    <li>Devised a hierarchical co-attention network to improve the AVS system’s adaptability to queries of variable length.</li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Explainable Visual Question Answering, 2016.08 - 2017.02
							</h4>
							<p>
								Supervised by Dr. Tao Mei in Microsoft Research Asia and Prof. Jiebo Luo
                                <ul>
                                    <li>Proposed a novel framework towards explainable VQA. Our framework can generate attributes and captions for images to explain why the system predicts the specific answer to the question.</li>
                                    <li>Defined four measurements of the explanations quality and demonstrated strong relationship between the explanations quality and the VQA accuracy. Our current system achieves comparable performance to the state-of-the-art and can improve with explanations quality.</li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Action and Activity Recognition in Video, 2014.12 - 2015.07
							</h4>
							<p>
								Supervised by Dr. Ting Yao, Dr. Tao Mei in Microsoft Research Asia and Prof. Jiebo Luo
                                <ul>
                                    <li>Proposed a hybrid framework to learn a deep multi-granular spatio-temporal representation for video action recognition by using 2D/3D CNNs and LSTM. Our paper is accepted and selected into the Best Paper Finalist by ICMR 2016 (Accepted Rate: 17%, Best Paper Finalist Rate: 1%). An improved version of the conference paper is accepted by IJMIR 2017.</li>
                                    <li>Won 2nd place in the Action Classification Task of THUMOS Challenge and presented our work on CVPR THUMOS Workshop in Boston, June 2015. This challenge contains over 430 hours of video data and 45 million frames on 101 action classes.</li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
            <!-- row -->
			<div class="row" style="margin-left:10px">
				<div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">					
					<div class="col-sm-9 col-md-9 col-lg-9">
						<blockquote style="margin-bottom:5px">
							<h4>
								Highlight Detection for First-Person Video Summarization, 2014.07 - 2014.12
							</h4>
							<p>
								Supervised by Dr. Ting Yao and Dr. Tao Mei in Microsoft Research Asia
                                <ul>
                                    <li>Collected a new large dataset from YouTube for first-person video highlight detection. The dataset consists of 100 hours videos mainly captured by GoPro cameras for 15 sports-related categories.</li>
                                    <li>Proposed a pairwise deep ranking model to detect highlight segments in videos. My contribution focuses on devising a two-stream CNN (frame and flow) to extract features for video segments.</li>
                                </ul>
							</p>
						</blockquote>
					</div>
				</div>
			</div>
			<!-- ./row -->	
            <br>
		</div>
	</section>
	<!-- /.research experiences -->
    
	<!-- footer -->
	<footer id="section-footer">
		<div class="container">
			<div class="row">
				<div class="col-md-12 col-lg-12">
					<div class="wow fadeIn animated" data-wow-delay="0.4s" style="visibility: visible;-webkit-animation-delay: 0.4s; -moz-animation-delay: 0.4s; animation-delay: 0.4s;">
						<div class="btn-circle-scroll">
							<a href="http://liqing-ustc.github.io#header-banner" class="btn-circle">
								<i class="fa fa-angle-double-up animated"></i>
							</a>
						</div>
					</div>
					<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=1BJXMdIkZWruyZ4HJBBYtRfZVexiUFfUmbIvIADaVb8&cl=ffffff&w=300"></script><br>
					<a href="https://www.easycounter.com/">
						<img src="https://www.easycounter.com/counter.php?liqing"
						border="0" alt="Free Hit Counter"></a> unique visitors since March 22, 2018.
				</div>
			</div>	
		</div>
	</footer>
	<!-- /.footer -->
	
	
	<!-- Core JavaScript Files -->
	<script src="./assets/jquery.min.js.download"></script>
	<script src="./assets/bootstrap.min.js.download"></script>
	<script src="./assets/jquery.easing.min.js.download"></script>	
	<script src="./assets/jquery.scrollTo.js.download"></script>
	<script src="./assets/wow.min.js.download"></script>			<!-- Reveal animation when you scroll by wow.js. It need animate.css library -->
	<!-- Custom Theme JavaScript -->
	<script src="./assets/custom.js.download"></script>




<div id="point-jawn" style="z-index: 2147483647;"></div></body></html>
