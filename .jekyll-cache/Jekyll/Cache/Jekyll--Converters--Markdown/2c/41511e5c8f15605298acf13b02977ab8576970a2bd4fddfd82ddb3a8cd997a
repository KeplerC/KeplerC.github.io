I"º<h2 id="regression">Regression</h2>
<p><em>K-fold cross-validation</em> randomly splits the training set into 10 distinct subsets called folds and train 9 folds and evaluate 1 fold.</p>

<h2 id="classification">Classification</h2>
<p>Classification using MNIST dataset</p>

<h3 id="dataset">Dataset</h3>
<p>this can be downloaded through sk learn 
the dictionary structure has 
DESCR : discription
data : array  row:instance column:feature 
target : labels</p>

<p>The previous parts of sections introduce classification accuracy: 
it has confusion matrix, which has a clean prediction(never see the data during training) on big fours</p>

<p>I ran into problems (I havenâ€™t figure out why), it says training error, although I just changed a straight line code into a class based one</p>

<p>Then I ran a one-hundred fold cross value set evaluation 
the time took super long, because it has to train it one by one 
The result sticks to 95%
[ 0.96672213  0.93843594  0.91347754  0.90848586  0.96505824  0.95673877
  0.97171381  0.95174709  0.93843594  0.91846922  0.89351082  0.95507488
  0.91181364  0.95341098  0.93178037  0.95008319  0.91680532  0.96339434
  0.94675541  0.95174709  0.9484193   0.95        0.94666667  0.93
  0.92166667  0.965       0.96333333  0.96666667  0.95666667  0.95333333
  0.97333333  0.96333333  0.95833333  0.865       0.96333333  0.965
  0.96833333  0.94833333  0.96333333  0.89        0.89666667  0.91666667
  0.94        0.95833333  0.96        0.94333333  0.965       0.92166667
  0.96833333  0.95        0.965       0.97833333  0.96833333  0.97
  0.97166667  0.97666667  0.96833333  0.96166667  0.985       0.94666667
  0.95333333  0.95333333  0.96        0.97        0.95666667  0.96666667
  0.96333333  0.97333333  0.97833333  0.96333333  0.95833333  0.965
  0.95333333  0.96333333  0.95666667  0.965       0.96166667  0.97
  0.97166667  0.96327212  0.95492487  0.97328881  0.95993322  0.96160267
  0.96994992  0.96327212  0.95659432  0.96661102  0.95826377  0.97495826
  0.95659432  0.94657763  0.95492487  0.95993322  0.96661102  0.95492487
  0.96327212  0.96661102  0.95659432  0.95158598]</p>

<p>### Confusion matrix 
  confusion matrix: the general idea is to count the number of times instances of class A are classified as class B.</p>

<p>Here we use</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_predict</span>
<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">sgd_clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_5</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>
<p>to perform a N0fold cross validation. As noted, this is a clean prediction</p>

<p>With this number, we can evaluate, precision, recall, true positive and etc</p>

<p>for example, to evaluate F1 score:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="n">f1_score</span><span class="p">(</span><span class="n">y_train_5</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>To extend your life, you need to recall all 10 surprising events from your memory. So, recall is the ratio of a number of events you can correctly <strong>recall</strong> to a number of all correct events.
precision is a measure of how many positive predictions were actual positive observations. It is that events you can correctly recall to a number all events you recall 
<a href="https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall">Interesting explanation</a></p>
</blockquote>

<h3 id="choosing-a-threshold">Choosing a threshold</h3>
<p>There is a tradeoff between precision and recall. The more recall(lower the threshold), the lower the precision is.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_scores</span> <span class="o">=</span> <span class="n">sgd_clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([</span><span class="n">some_digit</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">array</span><span class="p">([</span> <span class="mf">161855.74572176</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y_some_digit_pred</span> <span class="o">=</span> <span class="o">**</span><span class="p">(</span><span class="n">y_scores</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">)</span><span class="o">**</span>
<span class="n">array</span><span class="p">([</span> <span class="bp">True</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
</code></pre></div></div>

<p>I ran into the problem(as most of people did) about dimensionality</p>

<p>then I went to the issues on github and found 
one used 
y_scores = y_scores[:,1]
to reduce the dimensionality and solved it about the line</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_scores</span> <span class="o">=</span> <span class="n">cross_val_predict</span><span class="p">(</span><span class="n">sgd_clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train_5</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="n">method</span><span class="o">=</span><span class="s">"decision_function"</span><span class="p">)</span>
</code></pre></div></div>
<blockquote>
  <p>I believe whatâ€™s supposed to happen is that when SGDClassifier has 2 classes its decision function returns a single dimension arrray, but when there are greater than 2 classes it returns an ndarray where the first column signifies the class. In my instance 0 is supposed to be the binary false.
see in https://github.com/ageron/handson-ml/issues/81</p>
</blockquote>

<p>After this problem, everything is normal</p>

<p>**One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. **</p>

<h3 id="multi-class-classification">Multi-class classification</h3>
<p>OvO one vs one and do binary classification 
OvA one vs all(rest)</p>

<p>for N classes, algorithm needs to train N*(N-1) binary classifiers</p>

<h4 id="error-analysis">Error Analysis</h4>
<p>as we have a confusion matrix, we can visualize by</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">conf_mx</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p>Then we compute error rates by dividing number of images(like normalizing)</p>

<h3 id="knn--multi-label-classification">KNN &amp; Multi-label classification</h3>
<p>for multi output classification, like recognizing how many people on a photo, we can sue knn to solve that problem.</p>
:ET