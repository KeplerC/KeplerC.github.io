I"ö<h1 id="a-study-of-backpropagation">A study of Backpropagation</h1>
<p>As usual, the definition of backpropagation algorithm on wikipedia is</p>
<blockquote>
  <p>Backpropagation is a method used in artificial neural networks(ANN) to calculate the error contribution of each neuron after a batch of data (in image recognition, multiple images) is processed.</p>
</blockquote>

<h3 id="artificial-neural-network">Artificial Neural Network</h3>
<p>To explain ANN, it is a multilayered neural network under supervised learning and find best maps of a set of input to their correct output.</p>

<h3 id="loss-function">Loss Function</h3>
<p>It is just like reconstruction error, or cost function, that calculates the network output and expected output.</p>

<blockquote>
  <p>The input vector is propagated forward through the network, layer by layer, until it reaches the output layer. The output of the network is then compared to the desired output using loss function. The resulting error value is calculated for each of the neuron in the output layer.</p>
</blockquote>

<h3 id="contribution-of-each-neuron">Contribution of each neuron</h3>
<p>This algorithm has two phases: propagation and weight update.</p>

<h4 id="propagation">Propagation</h4>
<p>This step is to calculate a loss function and use error values to calculate gradient of loss function.</p>

<h4 id="weight-update">Weight Update</h4>
<p>Known the gradient of loss function, the algorithm updates weights to minimize it.</p>

<h4 id="hadamard-project">Hadamard project</h4>
<p>It is a binary operation looks like a âŠ™ b. It is element-wise or entry-wise operation, which can take place in two m*n matrices.</p>

<h2 id="intuition">Intuition</h2>
<p>Backpropagation is to calculate partial derivative of cost function w.r.t weight or w.r.t bias. The idea for backpropagation is that if one can minimize the cost function by moving the weight/bias to the opposite sign of the cost functionâ€™s derivative. Backpropagation offers a way to compute the loss functionâ€™s partial derivative of each neurons.</p>

<h2 id="equations">Equations</h2>
<p>The equations for backpropagation are presented in <a href="http://neuralnetworksanddeeplearning.com/chap2.html">1</a>
Inserting equations here seems to be complicated.</p>

<p>But I will still present a pseudo code similar to Professor Andrew Ng:</p>

<p>For each pair{x, y} in training set:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. set a(1) = x
2. perform forward propagation to compute activation level a in level 2-L, where L is the total number of layers 
3. using difference of a(L) and y to compute loss function
4. compute backward 
5. update weight by accumulating partial derivatives
</code></pre></div></div>

<h2 id="references">References</h2>
<p>https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</p>

<p>http://neuralnetworksanddeeplearning.com/chap2.html</p>

<p>Backpropagation algorithm, Backpropagation intuition Andrew Ng, coursera.org</p>

<p>Wikipedia</p>
:ET