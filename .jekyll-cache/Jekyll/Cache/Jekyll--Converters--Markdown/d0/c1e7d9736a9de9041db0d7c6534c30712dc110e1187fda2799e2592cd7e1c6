I"þ	<h1 id="notes-from-beginningii">Notes From Beginning(II)</h1>

<h2 id="classification">Classification</h2>
<p>Classification is different from Regression such that it is not actually a linear function. It depends on small number of labeled discrete values.</p>

<h3 id="hypothesis-representation">Hypothesis Representation</h3>
<p>To represent 0&lt;h_\sigma(x) &lt; 1, we can pass the equation \theta^T x into a <strong>logistic function</strong> g, such that h_theta(x) = g(\theta^Tx), where z = \theta^Tx, g(z) = \frac{1}{1+e^{-z}}
Then this Sigmoid Function can map any real number to [0,1]</p>

<h5 id="relation-to-probability">Relation to probability</h5>
<p>This h_\theta(x) gives the probability:</p>
<blockquote>
  <p>h_\theta(x) = 0.7 gives that it has 0.7â€™s probability to output 1. 
h_\theta(x) represents the probability that outputs 1</p>
  <h3 id="decision-boundary">Decision Boundary</h3>
  <p>For discrete binary classification that output 0 and 1, 
h_theta &gt; 0.5 -&gt; y=1
h_theta &lt; 0.5 -&gt; y=0
Luckily, we have z = 0, to have g(z) = 0.5 and the rest is fixed for h_\theta(x).</p>
</blockquote>

<h3 id="cost-function">Cost Function</h3>
<p>We cannot use the cost function in linear regression, 
then it is not a convex function, then J has many local maximum/minimums.</p>

<p>cost = -log(h_\theta(x)) if y = 1  (1)
and  = -log(1-h_\theta(x)) if y = 0 (2)
then it can be convex and have a global max/min</p>

<h4 id="intuition">Intuition</h4>
<p>Cost = 0: if y = 1 and h_\theta(x) = 1
while 
h_\theta(x) \rightarrow 0, then cost \rightarrow \inf 
    a large penalty cost</p>

<h4 id="a-simplified-cost-function">A simplified Cost Function</h4>
<p>Cost (h_theta(x), y) = -y (1) - (1-y) (2)
and we can do gradient descent based on that</p>

<h4 id="multiclass-classification">Multiclass Classification</h4>
<p>We can divide y = {0 â€¦ n} problem into n+1 binary classification problems</p>

<h4 id="reducing-overfitting-problem">Reducing Overfitting Problem</h4>
<p>we can reduce the weight by adding some terms in J such that 
J = J + 1000 * \theta_3 + 1000 * \theta_4</p>

<h3 id="regularization">Regularization</h3>
<p>The specific equations can be found on the webpage. Because I cannot write equations here, using latex format is not quite reader friendly.</p>

<h4 id="linear-regression">Linear Regression</h4>
<p>It has two parts, unregularized \theta_0 and the rest and regularized the rest. We penalize each theta by a factor \lamda \ \m.<br />
The related normal equation can also do the same. 
It is a small number.</p>
:ET