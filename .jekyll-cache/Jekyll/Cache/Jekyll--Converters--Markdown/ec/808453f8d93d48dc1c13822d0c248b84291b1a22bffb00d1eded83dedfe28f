I"É<h1 id="semi-supervised-learningi-label-propagation">Semi-supervised Learning(I): Label Propagation</h1>

<h2 id="context-ssl">ContextÔºö SSL</h2>

<p>Given a relative small labelled dataset {(x,y)} and a large  unlabeled dataset {x}, one needs to devise ways to perform both from classification.</p>

<p>Traditional approach includes</p>

<ul>
  <li>GMM. Assume a Gaussian distribution and for unlabeled dataset
    <ul>
      <li>determine label of clusters by labelled examples</li>
      <li>use labelled examples to determine mixture model</li>
    </ul>
  </li>
  <li>hard EM self-training
    <ul>
      <li>build a classifier with labelled data, then classify unlabeled data. Then the classifier is retrained the procedure is continued.</li>
    </ul>
  </li>
</ul>

<p>Then graph-based models gain their popularity. I think this is one variant of soft EM, which assumes the graphical structure and filling labels in a separate step, which is described in my previous blog about missing data(kinda connecting the dots here).</p>

<h3 id="does-unlabeled-data-always-help">Does unlabeled data always help?</h3>

<p>No. People have long realized that training Hidden Markov Model with unlabeled data (the Baum-Welsh algorithm, which by the way qualifies as semi-supervised learning on sequences) can reduce accuracy under certain initial conditions.</p>

<h4 id="how-does-ssl-use-unlabeled-data">How does ssl use unlabeled data?</h4>

<p>It can modify or reprioritize hypotheses obtained from labeled data alone, in which case it can identify P(x).</p>

<ul>
  <li>For generative models, P(x, y) = P(y|x)P(x), thus influence the results</li>
  <li>discriminative models does not</li>
</ul>

<h4 id="intuitive-example-from-2">Intuitive example from [2]</h4>

<p><img src="https://raw.githubusercontent.com/KeplerC/keplerc.github.io/master/_posts/img/ssl.png" alt="png" /></p>

<h2 id="algorithms">Algorithms</h2>

<ul>
  <li>EM with generative mixtures</li>
  <li>self-training</li>
  <li>co-training</li>
  <li>transudative SVM</li>
  <li>graph based models</li>
</ul>

<h3 id="label-propagation">Label Propagation</h3>

<h5 id="assumption-like-knn-closer-data-points-tend-to-have-similar-class-labels">Assumption: like knn, closer data points tend to have similar class labels</h5>

<p>We have a fully connected graph where are data points, labelled and unlabeled. we propagate labels through edges. Larger edge weights allow labels to travel through more easily.</p>

<h4 id="algorithm-pseudocode">Algorithm Pseudocode</h4>

<p>Transition Matrix T_{ij} with probability of transiting from i to j</p>

<ol>
  <li>Propagate by f = Tf. Then the label propagate to neighbors.</li>
  <li>‚ÄúClamp labelled data‚Äù. i.e. filling f‚Äôs labelled entries</li>
  <li>Repeat until convergence</li>
</ol>

<h3 id="next-transition-matrix-formulation">Next: Transition Matrix Formulation</h3>

<h3 id="references">References</h3>

<p>A good thesis that describes everything in ssl: http://pages.cs.wisc.edu/~jerryzhu/pub/thesis.pdf</p>

<p>Comprehensive ssl survey http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf</p>

<p>Graph Construction</p>

<p>https://www.ijcai.org/Proceedings/15/Papers/619.pdf</p>
:ET