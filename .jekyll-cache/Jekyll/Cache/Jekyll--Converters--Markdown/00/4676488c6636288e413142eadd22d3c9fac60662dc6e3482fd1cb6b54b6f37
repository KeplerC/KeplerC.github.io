I"$<h1 id="notes-from-beginningv">Notes From Beginning(V)</h1>
<h2 id="support-vector-machinesvm">Support Vector Machine(SVM)</h2>
<h3 id="optimization-objective">Optimization Objective</h3>
<p>Alternative view of logistic regression: 
if y=1, we want h(x) = 1, which \theta<em>x »0
The cost function for y=1 is -log(1/1+e^-z)
    we can simplify the curve by two lines and make them function cost_1(theta</em>.x) and cost_0
then the support vector machine is</p>

<p>min_\theta (1/m) \sum {y*cost_1 + (1-y) cost_0} + regularization term 
we can omit 1/m because it does not influence result</p>

<p>and a readjustment to regularization term to 
min_\theta (1/m) <strong>C</strong>* \sum {y*cost_1 + (1-y) cost_0} + regularization term</p>

<p>C = 1\lambda</p>

<p>for the appendix term theta^2 
we use theta^T theta</p>

<h3 id="intuition-large-margin-intuition">Intuition: large margin intuition</h3>
<p>Because of the boundary we draw, we cannot use the results 
-\eps &lt; 0 &lt; \eps and we want to judge the result beyond this range 
then the C * 0 = 0</p>

<p>if C is large, then it will be greatly influenced by outliers 
As a result, there is a tradeoff between bias and variance 
C(small lambda)   large       low bias, high variance
                  small       high bias, low variance 
variance          large       high bias, low variance: features will vary smoothly</p>

<h4 id="mathematics-behind">mathematics behind</h4>
<p>we convert theta*x to a dot product such taht 
p * ||theta|| &gt; 1 if y = 1 
p * ||theta|| &lt;= -1 if y = 1 
to minimize ||theta|| 
we need to maximize the projection p of x into vector theta</p>

<h3 id="kernel">Kernel</h3>
<p>a complex equation(like high order equation) will be computationally expensive, so we use the proximity of landmarks 
which similarity function is called kernel, 
k(x, l^i)
i is the ith landmark 
which is computed by Gaussian kernel 
k = exp(- Euclidean distance/ 2*variance^2)</p>

<h4 id="where-do-we-get-landmarks">Where do we get landmarks</h4>
<p>we use every training example as landmarks 
and have m similarity functions. 
Then for every x, we generate a <strong>feature vector</strong> that 
contains m’s similarity components of these similarity equations</p>

<p>then we no longer use polynomials theta^T x but feature vector f^Tx</p>

<h3 id="in-practice">In practice</h3>
<h4 id="specifics">specifics</h4>
<p>one need to specify
    choice of parameters C 
    choice of kernel(similarity function)
        no kernel == linear kernel when y=1 if theta^T &gt;= 0
        Gaussian Kernel, then need to choose variance 
            do not make feature scaling on it(metric different)
        Mercer’s theorem</p>

<h4 id="multi-class-classification">multi-class classification</h4>
<p>Use existing or one-vs-all</p>

<h4 id="choosing-between-logistic-regression-and-svm">Choosing between logistic regression and SVM</h4>
<p>when n is large: use logistic or SVM without kernel 
if n is small m is intermediate, use SVM is Gaussian kernel 
    (when m is 10-10 000)
if n is small m is large, add more features or use both</p>
:ET