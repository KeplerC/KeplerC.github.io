I"‹<h1 id="neural-translation-of-musical-style">Neural Translation of Musical Style</h1>
<p>This is a model that can learn to perform sheet music like human does, which can pass ‚Äúmusical Turing tests‚Äù. The key for just playing a note and human-performed music is a global structure and dependency of music. 
This is why we have LSTM to solve it. RNN: of course it has vanishing gradient problem.</p>

<h3 id="architecture-genrenet">architecture: GenreNet</h3>
<p>Sheet music -&gt; birectional LSTM -&gt; linear layer -&gt;dyanmics</p>
<ul>
  <li>bidirectional LSTM: ‚Äúcan look ahead‚Äù</li>
  <li>linear layer: activation function</li>
</ul>

<p>StyleNet: rendition model that can play variety of styles 
    Interpretation layer: converts musical input into own representation and then go through bidirectional LSTM</p>

<h3 id="implementaiton-details">implementaiton details</h3>

<h1 id="thoughts">thoughts</h1>
<p>One of our experiment as previous encoder-decoder neural network is this ‚Äúmusical turing tests‚Äù. We produce sounds and let them pass ‚Äúsound turing test‚Äù.</p>

<p>This model is less random than other implementation that I read. I think it is because the music itself is more structured(by sheet music, for example) and the model does not have so many factors to influence the fluency of music.</p>

<p>If I really wanted to implement a model that 
Still it is pretty good to read about their source code on 
<a href="https://github.com/imalikshake/StyleNet">github</a></p>
:ET